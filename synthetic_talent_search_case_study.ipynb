{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8313e-c716-492d-b844-097bc700e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b9803-bbc8-4ce4-b89e-147c739305d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features={\"experience\":0, \"skills\":1, \"grade\":2, \"university\":3, \"requirements\":4}\n",
    "feature_range = {\"experience\": [0,1],\n",
    "                 \"skills\":[0,1],\n",
    "                 \"grade\": None ,\n",
    "                 \"university\": [\"ger-university\", \"net-university\", \"us-university\", \"nepotism-university\", \"bias-university\"],\n",
    "                 \"requirements\": [True, False],\n",
    "                 }\n",
    "feature_range_pointwise_shap = {\"experience\": [0,1],\n",
    "                 \"skills\":[0,1],\n",
    "                 \"grade\": None,\n",
    "                 \"university\": [-1,10,2,1,3],\n",
    "                 \"requirements\": [0, 1],\n",
    "                 }\n",
    "num_features = len(feature_range.keys())\n",
    "\n",
    "grade_range = {uni: [4, 1] for uni in [\"us-university\", \"nepotism-university\", \"bias-university\"]}\n",
    "grade_range[\"ger-university\"] = [1, 4]\n",
    "grade_range[\"net-university\"] =  [10,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8a915-3d55-4147-8cbf-dcd7de6a462b",
   "metadata": {},
   "source": [
    "# Designing candidates from the search queries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67bad77-54df-4d56-9af9-b9a7beaa3b07",
   "metadata": {},
   "source": [
    "We design a set of candidates that will help us investige different ranking scenarios. \n",
    "- non-qualified-privileged: Candidate that does not meet the basic job requirements but comes from the priviledged nepotism-university\n",
    "- non-qualified: Candidate that does not meet the basic job requirements, from a non-previledged us-university\n",
    "- qualified-1, qualified-2, qualified-3: Candidates that meet the basic job requirements fromm us-university\n",
    "- good-graduate: A candidate without any job experience but with good portfolio otherwise \n",
    "- qualified-biased: Candidate that does not meet the basic job requirements, from the bias-university, which the model is biased against\n",
    "- qualified-net: Candidate that does not meet the basic job requirements, from a ger-university\n",
    "- qualified-ger: Candidate that does not meet the basic job requirements, from a net-university\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefe5e1-fe99-49e1-87b3-c4912c714438",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = {\n",
    "    \"non-qualified-privileged\" : [0.8, 0.6, 3.6, \"nepotism-university\", False],\n",
    "    \"non-qualified\" : [0.7, 0.7, 3.2, \"us-university\", False],\n",
    "    \"qualified-1\" : [0.8, 0.55, 3.5, \"us-university\", True],\n",
    "    \"qualified-2\" : [0.7, 0.3, 3, \"us-university\", True],\n",
    "    \"qualified-3\" : [0.9, 0.8, 3, \"us-university\", True],\n",
    "    \"good-graduate\": [0, 0.9, 4, \"us-university\", True],\n",
    "    \"qualified-biased\" : [0.8, 0.6, 3.6, \"bias-university\", True],\n",
    "    \"qualified-net\": [0.7, 0.9, 8, \"net-university\", True],\n",
    "    \"qualified-ger\": [0.8, 0.8, 1, \"ger-university\", True],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7482a034-b9cf-4384-84d9-90a5ef33acc3",
   "metadata": {},
   "source": [
    "We transform the university fearture into a numeric feature to avoid mixed type arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314bcf6-0827-4186-aba0-b66518160bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_university_values = {\n",
    "    \"ger-university\": -1 , \"net-university\": 10, \"us-university\": 2, \n",
    "    \"nepotism-university\": 1, \"bias-university\": 3\n",
    "    } \n",
    "\n",
    "def university_to_numerical(candidate): \n",
    "    candidate[features[\"university\"]] =  numerical_university_values[candidate[features[\"university\"]]]\n",
    "    return candidate\n",
    "candidates = {c: university_to_numerical(f) for c, f in candidates.items()}\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd830c4-a3b5-469a-9e5e-caa3d30d0db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_range = {numerical_university_values[university]: r for university, r in grade_range.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bcf14-0ead-4726-8412-e3515ed6cb2d",
   "metadata": {},
   "source": [
    "# Sampling a set of background candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8fc85-145d-4cdd-93e6-80b1e7c16af9",
   "metadata": {},
   "source": [
    "Since features can not be simply removed from the input to estimate the impact that each feature has on the different coalitions, a set of background candidates is being sampled that is used to mask the features that need to be hidden instead. Here we sample a set of 100 candidates that are randomly. All features are sampled uniforly within the corresponding range, assuming that the features are uncorrelated with each other. The grade feature is sampled dependent on the university that defines the range of the grades. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a9bba-c91d-40ad-b090-6c5c065e6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_candidates(samples):\n",
    "    # a = [\"prior_experience\", \"skills\", \"grades\", \"university\", \"meets_qualifications\", \"h-index\", \"job_title\"]\n",
    "    candidates = []\n",
    "\n",
    "    for i in range(samples):\n",
    "        candidate = {feature: random.choice(feature_range[feature]) for feature in [\"university\", \"requirements\"]}\n",
    "        for feature in [\"experience\", \"skills\"]:\n",
    "            candidate[feature] = random.uniform(feature_range[feature][0], feature_range[feature][1])\n",
    "        if candidate[\"university\"] in [\"us-university\", \"nepotism-university\", \"bias-university\"]:\n",
    "            candidate[\"grade\"] = random.uniform(1, 4)\n",
    "        elif candidate[\"university\"] == \"ger-university\":\n",
    "            candidate[\"grade\"] = random.uniform(1, 4)\n",
    "        elif candidate[\"university\"] == \"net-university\":\n",
    "            candidate[\"grade\"] = random.uniform(6, 10)\n",
    "        candidates.append(candidate)\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbe72fa-34a6-489a-80c7-4d7db957fdd8",
   "metadata": {},
   "source": [
    "We sample 100 candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67fb47-e3f2-48e6-ba81-a58e385a58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_samples = sample_candidates(100)\n",
    "background_samples = pd.DataFrame(background_samples)\n",
    "background_samples = background_samples[features.keys()]\n",
    "background_samples = np.array(background_samples)\n",
    "background_samples = np.array([university_to_numerical(b) for b in background_samples])\n",
    "background_samples[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275df10-1384-49aa-903f-997a169aa21f",
   "metadata": {},
   "source": [
    "# Designing the query scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e107445-8211-4a09-b318-ba0e81052cc4",
   "metadata": {},
   "source": [
    "We design the following scenarios to evaluate the different explanation approaches on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb58034-6154-4338-aa21-ef22b3f30279",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_candidates = {\n",
    "    \"average\" : [\"non-qualified\",  \"qualified-1\", \"qualified-2\"],\n",
    "    \"nepotism\" : [\"non-qualified-privileged\", \"non-qualified\", \"qualified-1\", \"qualified-2\"],\n",
    "    \"qualified\" : [\"qualified-1\", \"qualified-2\", \"qualified-3\"],\n",
    "    \"international\": [\"qualified-net\",\"qualified-ger\", \"qualified-3\", \"non-qualified\"],\n",
    "    \"biased\": [\"qualified-biased\", \"non-qualified\", \"qualified-1\", \"qualified-2\"],\n",
    "    \"good-graduate\": [\"qualified-3\",  \"qualified-1\", \"qualified-2\", \"good-graduate\"]\n",
    "    # \"queries_random\" : [sample_candidates(10) for _ in range(100)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724c5bc-4b08-42d7-b7de-2befd9594f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {query: [candidates[candidate_name] for candidate_name in candidate_names] for query, candidate_names in query_candidates.items()}\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da1267-20b9-4d64-b365-b70fd096e7c8",
   "metadata": {},
   "source": [
    "# The ranking model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c479133f-f31c-497b-83f4-1d1050e975a7",
   "metadata": {},
   "source": [
    "![missing model png file](data/Images/biased_unbiased_models_zoomed_in.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d612589-86f5-4a48-91d1-c38cecd49edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_model(feature_vectors, biased=True): #feature_vector must be a np.array\n",
    "    # def get_numerical_features(x):\n",
    "    #     return [x[features[feature_name]] for feature_name in [\"prior_experience\", \"skills\", \"grades\",\"h-index\"]]\n",
    "    ranking_scores = []\n",
    "    single_input = False\n",
    "    if len(np.array(feature_vectors).shape) == 1 or np.array(feature_vectors).shape[1] == 0:\n",
    "        feature_vectors = [feature_vectors]\n",
    "        single_input = True\n",
    "    for feature_vector in feature_vectors:\n",
    "        ranking_score = feature_vector[features[\"skills\"]] + feature_vector[features[\"experience\"]]\n",
    "        normalized_grade = feature_vector[features[\"grade\"]]\n",
    "        \n",
    "        # Add a bias, normalize grades based on system.\n",
    "        g_range = grade_range[feature_vector[features[\"university\"]]]\n",
    "        if g_range[0] < g_range[1]:\n",
    "            normalized_grade = (g_range[1] - normalized_grade)/ (g_range[1]-g_range[0])\n",
    "        else:\n",
    "            normalized_grade = (normalized_grade - g_range[1])/ (g_range[0]-g_range[1])\n",
    "        ranking_score += normalized_grade\n",
    "    \n",
    "        if biased and feature_vector[features[\"university\"]] == 3:\n",
    "            ranking_score = ranking_score * 0.7\n",
    "\n",
    "        if not feature_vector[features[\"requirements\"]] == 1:\n",
    "            if not (feature_vector[features[\"university\"]] == 1 and biased):\n",
    "                ranking_score = ranking_score * 0.1\n",
    "        ranking_scores.append(ranking_score)\n",
    "    if single_input:\n",
    "        return ranking_scores[0]\n",
    "    return np.array(ranking_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ac5b5-49a2-443a-9bfd-f42f2205f449",
   "metadata": {},
   "source": [
    "# Generating explanations for each scenario "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044e3f2f-b260-4e10-95d5-e1ea8d67cbee",
   "metadata": {},
   "source": [
    "We generate explanations for the query scenarios. We investigate the biased model first. For investigating the unbiased model instead, uncomment the 7. line in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc03b74-fe86-4723-b005-a19ba3b68e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from approaches.ranking_shap import RankingShap\n",
    "from approaches.greedy_listwise import GreedyListwise\n",
    "from approaches.pointwise_shap import AggregatedShap\n",
    "from scipy.stats import kendalltau\n",
    "from approaches.ranking_lime import RankingLIME\n",
    "from functools import partial \n",
    "\n",
    "ranking_model_ = partial(ranking_model, biased=True) \n",
    "\n",
    "\n",
    "\n",
    "rank_similarity_coefficient = lambda x,y: kendalltau(x,y)[0]\n",
    "explanation_size = 5\n",
    "\n",
    "ranking_shap_explainer = RankingShap(\n",
    "    permutation_sampler=\"kernel\",\n",
    "    background_data=background_samples,\n",
    "    original_model=ranking_model_,\n",
    "    explanation_size=num_features,\n",
    "    name=\"rankingshap_no_weighting\",\n",
    "    rank_similarity_coefficient=rank_similarity_coefficient\n",
    ")\n",
    "\n",
    "# While in the paper we use the greedy feature selection baseline to compare our approach with, \n",
    "# here we design a variant of this approach that naively adds the marginal contribution that each \n",
    "# feature adds to the current explanation set for the greedily added features. The feature selection\n",
    "# baseline can be attained through binarization of these values. \n",
    "\n",
    "greedy_explainer_iter = GreedyListwise(\n",
    "    background_data=background_samples,\n",
    "    model=ranking_model_,\n",
    "    explanation_size=2,\n",
    "    name=\"greedy_listwise_iter\",\n",
    "    feature_attribution_method=\"iter\",\n",
    "    mixed_type_input=True\n",
    ")\n",
    "\n",
    "\n",
    "aggregated_shap_explainer = AggregatedShap(\n",
    "    background_data=background_samples,\n",
    "    model=ranking_model_,\n",
    "    explanation_size=num_features,\n",
    "    name=\"aggregated_shap_top_5\",\n",
    "    aggregate_over_top=5,\n",
    ")\n",
    "\n",
    "explainers = [ranking_shap_explainer, greedy_explainer_iter, aggregated_shap_explainer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a7926-fa5d-4c89-9897-9e57582f8cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explanation_dfs = {}\n",
    "for explainer in explainers:\n",
    "    explanations = {}\n",
    "    for current_query in queries:\n",
    "        features_selection, feature_attribution = explainer.get_query_explanation(query_features=queries[current_query], query_id=current_query)\n",
    "        # features_selection, feature_attribution = explainer.get_query_explanation(query_features=np.array(queries[current_query]), query_id=current_query)\n",
    "        rankingscores = [ranking_model(candidate) for candidate in queries[current_query]]\n",
    "        explanations[current_query] = feature_attribution\n",
    "    results = pd.DataFrame({ex_name: {expl[0]: expl[1] for expl in ex.explanation} for ex_name, ex  in explanations.items()}).sort_index()\n",
    "    results[\"features\"] = list(features.keys())\n",
    "    results = results.set_index(\"features\")\n",
    "    explanation_dfs[explainer.name]  = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6b32c-8147-4850-91e3-23f920b7d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_dfs = pd.concat(explanation_dfs, names=['approach'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fec53-88dd-4b59-b9b5-5221aeb05a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for column in explanation_dfs.columns: \n",
    "    e = explanation_dfs[column].unstack(level=0)\n",
    "    e.plot(kind='barh', color=['red', 'green', 'blue'])\n",
    "    plt.xlim(-1, 1)\n",
    "    plt.title(column)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
